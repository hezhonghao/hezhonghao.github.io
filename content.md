
<div class="center-header">

# Zhonghao He (何忠豪)

</div>

Hi! I am Zhonghao, currently a master's student at Cambridge and an incoming research intern at [FLAIR](https://foersterlab.com/), Oxford. I build AI for collaborative truth-seeking and moral progress. Two of the works I co-led on this topic are accepted ICML 2025 and NeurIPS 2025. I am seeking research & PhD positions! My [CV](Zhonghao_CV_2025.pdf) here.

[Email](mailto: hezhonghao2030@gmail.com) / [Google Scholar](https://scholar.google.com/citations?user=PuUcZTYAAAAJ&hl=en&oi=ao) / [Github](https://github.com/hezhonghao) / [Twitter](https://x.com/zhonghaohe)

I am serving as a mentor at the Supervised Program for Alignment Research and the Algoverse AI Safety Fellowship. If you'd like to informally work with me, you may check out our [idea portal](https://docs.google.com/document/d/17HGZ8M8QY5Lvna3Cxp83U6uXNMbnWRJWA1t3dE6yoco/edit?tab=t.0).

# **Research**

My current "[Hamming Problems](https://www.cs.virginia.edu/~robins/YouAndYourResearch.html)" (the most important problems I can work on) are about building AI for the collaborative truth-seeking and moral progress, specifically:

- **Truth-seeking AI**: Can we enforce Bayesian for arbitrary technical systems, be it single LLM reasoning, multi-agent systems, or human-AI systems? Can we construct truth proxy in open-ended domains (science & value-laden) where ground truth is non-existed? 

- **Human-AI Collaboration**: Can truth-seeking AI assistant help to reduce confirmation bias of humans? Can we evaluate LLM based on how much they help with humans' truth-seeking tasks? When information is ever accessible, how do we simutaneously build up individual and societal capacity for information processing and decision making?

- **Modeling of AI Influence & Moral Progress**: When advancing AI with narrowly defined capability metrics and excessive human approval, what would be downstream impact? When the feedback loops between humans and AI are established and mutually reinforcing, should we worry about the permenant lock-in of false beliefs? Automation and material abundance are indeed desiderata, but what do we count as real progress?

[Here are most updated technical details of our ongoing research.](https://tinyurl.com/prevailai)


# **Selected Works**

> [Stay True to the Evidence: Measuring Belief Entrenchment in Reasoning LLMs via the Martingale Property](https://tinyurl.com/martingalescore) (NeurIPS'25)

> **Zhonghao He\***, Tianyi Qiu\*, Hirokazu Shirado, Maarten Sap (2025)
> (*Equal contribution)

If Bayesian reasoning is ideal, can we develop unsupervised methods that provides helpful training signals?

We introduce the Martingale Score, an unsupervised metric based on Bayesian statistics, to show that the deviation from Bayesian reasoning can be measured by predictability of belief update, based on solely the prior. reasoning in LLMs often leads to belief entrenchment rather than truth-seeking, and shows this score predicts ground-truth accuracy. 

Martingale property: under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief ([see theoretical underpinning](https://arxiv.org/abs/2109.07007)).

The first paper in this line of work was accepted by NeurIPS 2025. Now we extend the same metric to human-AI interaction, multi-agent systems, and to use it as a training remedy to belief entrenchment in LLMs and confirmation bias in humans. Specific technical details can be seen [here](https://docs.google.com/document/d/1rHhOVqLlEMwZYJ7p520P9Qctjj52LlU0y6tza32xENo/edit?tab=t.5f1zeybxos72#heading=h.ph9x87vdlmn3).

>[The Lock-in Hypothesis: Stagnation by Algorithm](https://thelockinhypothesis.com) (ICML'25)

>Tianyi Qiu\*, **Zhonghao He\***, Tejasveer Chugh, Max Kleiman-Weiner (2025)
>(*Equal contribution)

When AI mediates most of our information intake and learning while being anything but a truth-seeker, what would be the lont-term consequences. 

Introducing the lock-in hypothesis: The feedback loop in humanAI interaction will eventually lead a population to converge on false beliefs. Such beliefs, once formed, are hard to change with opposing evidence, as feedback loops indiscriminately amplify confidence in existing individual and collective beliefs and humans develop trust in AI.

We use formal methods, simulations, and hypothesis-testing to study the value lock-in problem, in a hope to connect human-AI interaction mechanisms to long-standing epistemic and morality problems facing our time.

# **Gratitude**

I am forever grateful to the mentorship of [Max Kleiman Weiner](https://faculty.washington.edu/maxkw/), [Maarten Sap](https://maartensap.com/), [Hirokazu Shirado](https://www.shirado.net/), [Henry Shevlin](https://henryshevlin.com/), [Grace W. Lindsay](https://gracewlindsay.com/), [Anna Ivanova](https://anna-ivanova.net/). They have played significant roles in supporting my academic career.

I strive to become a "full stack researcher," which, in my definition, is to have technical sophistication (experiments, mathematical formulation, and engineering) and deep engagements with problems (technical and societal ones). Building technologies for human betterment is hard, and let's get this one right.

Ultimately, I want to build AIs for human excellence (or "arete", in Greek conception) and moral progress, which requires both sound societal mechanism design and epistemic tools with which individuals can better exercise their agency.

A lot of effort is required to operationalize those concepts, but currently I am actively exploring the following topics: mechanistic interpretability, computational neuroscience, AI ethics, alignment, political philosophy, virtue ethics, multi-agent systems, AI for science, and human-computer interface, and collective intelligence.

# **Contacts**

I overall have a ["open-door" policy](https://www.cs.virginia.edu/~robins/YouAndYourResearch.html#:~:text=Another%20trait%2C%20it,they%20miss%20fame.), meaning that I am in general very open for people to book a slot on my calendar on Monday/Friday to discuss research/startups/education. [Calendly](https://calendly.com/hezhonghao). Besides, I am happy to give advice to people with disadvantaged backgrounds, especially if you are deciding to make a move from a non-technical background to a technical route, or your educational opportunities are systematically deprived. 
