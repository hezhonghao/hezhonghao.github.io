
<div class="center-header">

# Zhonghao He (何忠豪)

</div>

Hi! I am Zhonghao, currently a master's student at Cambridge and an incoming research intern at [FLAIR](https://foersterlab.com/), Oxford. I build AI for collaborative truth-seeking and moral progress. Two of the works I co-led on this topic are accepted ICML 2025 and NeurIPS 2025. I am seeking research & PhD positions! My [CV](Zhonghao_CV_2025.pdf) here.

[Email](hezhonghao2030@gmail.com) / [Google Scholar](https://scholar.google.com/citations?user=PuUcZTYAAAAJ&hl=en&oi=ao) / [Github](https://github.com/hezhonghao) / [Twitter](https://x.com/zhonghaohe)

I am serving as a mentor at the Supervised Program for Alignment Research and the Algoverse AI Safety Fellowship. If you'd like to informally work with me, you may check out our [idea portal](https://docs.google.com/document/d/17HGZ8M8QY5Lvna3Cxp83U6uXNMbnWRJWA1t3dE6yoco/edit?tab=t.0).

# **Research**

My current "[Hamming Problems](https://www.cs.virginia.edu/~robins/YouAndYourResearch.html)" (the most important problems I can work on) are about building AI for the collaborative truth-seeking and moral progress, specifically:

- **Truth-seeking AI**: Can we enforce Bayesian for arbitrary technical systems, be it single LLM reasoning, multi-agent systems, or human-AI systems? Can we construct truth proxy in open-ended domains (science & value-laden) where ground truth is non-existed? 

- **Human-AI Collaboration**: Can truth-seeking AI assistant help to reduce confirmation bias of humans? Can we evaluate LLM based on how much they help with humans' truth-seeking tasks? When information is ever accessible, how do we simutaneously build up individual and societal capacity for information processing and decision making?

- **Modeling of AI Influence & Moral Progress**: When advancing AI with narrowly defined capability metrics and excessive human approval, what would be downstream impact? When the feedback loops between humans and AI are established and mutually reinforcing, should we worry about the permenant lock-in of false beliefs? Automation and material abundance are indeed desiderata, but what do we count as real progress?

[Here are most updated technical details of our ongoing research](https://tinyurl.com/prevailai)


# **Selected Works**

> [Stay True to the Evidence: Measuring Belief Entrenchment in Reasoning LLMs via the Martingale Property](https://tinyurl.com/martingalescore) (NeurIPS'25)
> **Zhonghao He\***, Tianyi Qiu\*, Hirokazu Shirado, Maarten Sap (2025)
> (*Equal contribution)

We introduce the Martingale Score, an unsupervised metric based on Bayesian statistics, to show that reasoning in LLMs often leads to belief entrenchment rather than truth-seeking, and shows this score predicts ground-truth accuracy. 

The key property we rely on: under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief.

We think belief entrenchment is the major reason why LLMs are causing delusions and even psychosis in users today, and, if the root cause remains unaddressed, it may cause more serious epistemic harm at an even larger scale. Based on this work, we aim to extend the same metric to human-AI interaction, and to use it as a training remedy to belief entrenchment.

>[The Lock-in Hypothesis: Stagnation by Algorithm](https://thelockinhypothesis.com) (ICML'25)
>Tianyi Qiu\*, **Zhonghao He\***, Tejasveer Chugh, Max Kleiman-Weiner (2025)
>(*Equal contribution)

Risk (pre-mature value lock-in): Frontier AI systems hold increasing influence over the epistemology of human users. Such influence can reinforce prevailing societal values, potentially contributing to the lock-in of misguided moral beliefs and, consequently, the perpetuation of problematic moral practices on a broad scale. 

Solution (progress alignment): We introduce progress alignment as a technical solution to mitigate this imminent risk. Progress alignment algorithms learn to emulate the mechanics of human moral progress, thereby addressing the susceptibility of existing alignment methods to contemporary moral blindspots. 

Infrastructure (ProgressGym): To empower research in progress alignment, we introduce ProgressGym, an experimental framework allowing the learning of moral progress mechanics from history, in order to facilitate future progress in real-world moral decisions. Leveraging 9 centuries of historical text and 18 historical LLMs, ProgressGym enables codification of real-world progress alignment challenges into concrete benchmarks. (Hugging Face, GitHub, Leaderboard)

# ** Trajectory 
- Jan 2017 Read Life3.0, realised something is coming up and it takes this generation of people to make right decisions for AI to get right. 

- Aug 2019 Was confused about PhD VS startup, reached out John L. Hennessy, was told "Unless you have world-changing technology, do a PhD", started a company anyway, with non-world-changing technology.

- Jul 2021 Became a researcher when selected as a research fellow to Stanford Existential Risks Initiative (SERI).

- Mar 2022 Became first COO of Charity Box, now a leading effective donation advocacy institute.

- May 2022 Incubated the first batch of AI safety researchers in China, teaming up with SERI.

- Oct 2022 Started a master degree at Cambridge University.

- Feb 2023 First publication in life.

- (meanwhile, a lot of rejections ...)

- Mar 2025 First first-authored technical publication, made to ICML.

# **Gratitude**

I am forever grateful to the mentorship of [Max Kleiman Weiner](https://faculty.washington.edu/maxkw/), [Maarten Sap](https://maartensap.com/), [Hirokazu Shirado](https://www.shirado.net/), [Henry Shevlin](https://henryshevlin.com/), [Grace W. Lindsay](https://gracewlindsay.com/), [Anna Ivanova](https://anna-ivanova.net/). They have played significant roles in supporting my academic career.

I strive to become a "full stack researcher," which, in my definition, is to have technical sophistication (experiments, mathematical formulation, and engineering) and deep engagements with problems (technical and societal ones). Building technologies for human betterment is hard, and let's get this one right.

Ultimately, I want to build AIs for human excellence (or "arete", in Greek conception) and moral progress, which requires both sound societal mechanism design and epistemic tools with which individuals can better exercise their agency.

A lot of effort is required to operationalize those concepts, but currently I am actively exploring the following topics: mechanistic interpretability, computational neuroscience, AI ethics, alignment, political philosophy, virtue ethics, multi-agent systems, AI for science, and human-computer interface, and collective intelligence.

# **Contacts**

I overall have a ["open-door" policy](https://www.cs.virginia.edu/~robins/YouAndYourResearch.html#:~:text=Another%20trait%2C%20it,they%20miss%20fame.), meaning that I am in general very open for people to book a slot on my calendar on Monday/Friday to discuss research/startups. [Calendly](https://calendly.com/hezhonghao) <!---You may drop me an email at hezhonghao2030@gmail.com--->
