
Hi! I am Zhonghao, currently a master's student at the University of Cambridge and an incoming research intern at [FLAIR](https://foersterlab.com/), Oxford. I build AI for collaborative truth-seeking and moral progress. Two of the works I co-led on this topic are accepted ICML 2025 and NeurIPS 2025. I am seeking research & PhD positions! My [CV](Zhonghao_CV_2025.pdf) here.

[Email](hezhonghao2030@gmail.com) / [Google Scholar](https://scholar.google.com/citations?user=PuUcZTYAAAAJ&hl=en&oi=ao) / [Github](https://github.com/hezhonghao) / [Twitter](https://x.com/zhonghaohe)

I am serving as a mentor at the Supervised Program for Alignment Research and the Algoverse AI Safety Fellowship. If you'd like to informally work with me, you may check out our [idea portal](https://docs.google.com/document/d/17HGZ8M8QY5Lvna3Cxp83U6uXNMbnWRJWA1t3dE6yoco/edit?tab=t.0).

# **Research**

My current "[Hamming Problems](https://www.cs.virginia.edu/~robins/YouAndYourResearch.html)" (the most important problems I can work on) are about building AI for the collaborative truth-seeking and moral progress, specifically:

- **Truth-seeking AI***: Can we enforce Bayesian for arbitrary technical systems, be it single LLM reasoning, multi-agent systems, or human-AI systems? Can we construct truth proxy in open-ended domains (science & value-laden) where ground truth is non-existed? 

- **Human-AI Collaboration**: Can truth-seeking AI assistant help to reduce confirmation bias of humans? Can we evaluate LLM based on how much they help with humans' truth-seeking tasks? When information is ever accessible, how do we simutaneously build up individual and societal capacity for information processing and decision making?

- **Modeling of AI Influence & Moral Progress**: When advancing AI with narrowly defined capability metrics and excessive human approval, what would be downstream impact? When the feedback loops between humans and AI are established and mutually reinforcing, should we worry about the permenant lock-in of false beliefs? Automation and material abundance are indeed desiderata, but what do we count as real progress?

[Technical details of Ongoing Research](https://tinyurl.com/prevailai)


# Selected Works 


- Will humanity experience an LLM-induced value lock-in because of the mutual learning feedback loops between humans and LLMs? (ICML 2025; Read [The Lock-in Hypothesis](https://thelockinhypothesis.com))
- Can we train truth-seeking AI when agentic AI starts to mediate almost all our information intakes? (NeurIPS 2025; Read [Martingale Score: An Unsupervised Metric for LLM Bayesian Reasoning](https://tinyurl.com/martingalescore))

> Stay True to the Evidence: Measuring Belief Entrenchment in Reasoning LLMs via the Martingale Property (NeurIPS'25)
> **Zhonghao He\***, Tianyi Qiu*, Hirokazu Shirado, Maarten Sap (2025)
> (*Equal contribution)

We introduce the Martingale Score, an unsupervised metric based on Bayesian statistics, to show that reasoning in LLMs often leads to belief entrenchment rather than truth-seeking, and shows this score predicts ground-truth accuracy. 

The key property we rely on: under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief.

We think belief entrenchment is the major reason why LLMs are causing delusions and even psychosis in users today, and, if the root cause remains unaddressed, it may cause more serious epistemic harm at an even larger scale. Based on this work, we aim to extend the same metric to human-AI interaction, and to use it as a training remedy to belief entrenchment.

>The Lock-in Hypothesis: Stagnation by Algorithm (ICML'25)
>Tianyi Qiu*, **Zhonghao He\***, Tejasveer Chugh, Max Kleiman-Weiner (2025)
>(*Equal contribution)

Risk (pre-mature value lock-in): Frontier AI systems hold increasing influence over the epistemology of human users. Such influence can reinforce prevailing societal values, potentially contributing to the lock-in of misguided moral beliefs and, consequently, the perpetuation of problematic moral practices on a broad scale. 

Solution (progress alignment): We introduce progress alignment as a technical solution to mitigate this imminent risk. Progress alignment algorithms learn to emulate the mechanics of human moral progress, thereby addressing the susceptibility of existing alignment methods to contemporary moral blindspots. 

Infrastructure (ProgressGym): To empower research in progress alignment, we introduce ProgressGym, an experimental framework allowing the learning of moral progress mechanics from history, in order to facilitate future progress in real-world moral decisions. Leveraging 9 centuries of historical text and 18 historical LLMs, ProgressGym enables codification of real-world progress alignment challenges into concrete benchmarks. (Hugging Face, GitHub, Leaderboard)

# **Gratitude**

I am forever grateful to the mentorship of [Max Kleiman Weiner](https://faculty.washington.edu/maxkw/), [Maarten Sap](https://maartensap.com/), [Hirokazu Shirado](https://www.shirado.net/), [Henry Shevlin](https://henryshevlin.com/), [Grace W. Lindsay](https://gracewlindsay.com/), [Anna Ivanova](https://anna-ivanova.net/). They have played significant roles in supporting my academic career.

I strive to become a "full stack researcher," which, in my definition, is to have technical sophistication (experiments, mathematical formulation, and engineering) and deep engagements with problems (technical and societal ones). Building technologies for human betterment is hard, and let's get this one right.

Ultimately, I want to build AIs for human excellence (or "arete", in Greek conception) and moral progress, which requires both sound societal mechanism design and epistemic tools with which individuals can better exercise their agency.

A lot of effort is required to operationalize those concepts, but currently I am actively exploring the following topics: mechanistic interpretability, computational neuroscience, AI ethics, alignment, political philosophy, virtue ethics, multi-agent systems, AI for science, and human-computer interface, and collective intelligence.

# **Contacts**

I overall have a ["open-door" policy](https://www.cs.virginia.edu/~robins/YouAndYourResearch.html#:~:text=Another%20trait%2C%20it,they%20miss%20fame.), meaning that I am in general very open for people to book a slot on my calendar on Monday/Friday to discuss research/startups. [Calendly](https://calendly.com/hezhonghao) <!---You may drop me an email at hezhonghao2030@gmail.com--->
